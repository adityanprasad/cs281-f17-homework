{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preliminaries\n",
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading data using numpy\n",
    "data = np.loadtxt('CASP.csv', delimiter=',', skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45730, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = data[:, 0]\n",
    "X = data[:,1:]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating training data - 90;10 split\n",
    "l = int(.9*Y.shape[0])\n",
    "\n",
    "Y_train = Y[:l]\n",
    "Y_test = Y[l:]\n",
    "X_train = X[:l]\n",
    "X_test = X[l:]\n",
    "\n",
    "assert X_train.shape[0]==.9*X.shape[0]\n",
    "assert X_test.shape[0]==0.1*X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing features.\n",
    "\n",
    "For each feature, $X_i$, we have $X_i = \\sigma  Z + \\mu \\implies Z =  \\frac{(X_i - \\mu)}{\\sigma}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41157, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalizing test data across each axis. This returns a vector with the mean of each column\n",
    "mu_train = X_train.mean(axis=0)\n",
    "var_train = X_train.var(axis=0)\n",
    "\n",
    "mu_test = X_test.mean(axis=0)\n",
    "var_test = X_test.var(axis=0)\n",
    "\n",
    "#print(mu_x.shape)\n",
    "\n",
    "# renaming normalized variables; there is numpy broadcasting magic going on here: mu_x has dim (9,) and X_train\n",
    "# has dim (41157,9) => X_train - mu_x will have dim (41157,9)\n",
    "Z = (X_train - mu_train)/np.sqrt(var_train)\n",
    "Z_test = (X_test - mu_test)/np.sqrt(var_test)\n",
    "Z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding bias feature (to first column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = np.ones((Z.shape[0], Z.shape[1]+1))\n",
    "D[:, 1:] = Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bias for test data\n",
    "D_test = np.ones((Z_test.shape[0], Z_test.shape[1]+1))\n",
    "D_test[:, 1:] = Z_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.39767095,  0.64721867, ..., -0.64707638,\n",
       "        -0.10005648,  0.7050291 ],\n",
       "       [ 1.        ,  0.29926327, -0.0535137 , ..., -0.08606622,\n",
       "        -0.98803634,  0.02224952],\n",
       "       [ 1.        ,  0.09910086, -0.38016704, ..., -0.11615054,\n",
       "        -0.5795656 , -0.22750538],\n",
       "       ..., \n",
       "       [ 1.        , -0.53920322, -0.37045806, ..., -0.35353095,\n",
       "        -0.43748883,  0.5025857 ],\n",
       "       [ 1.        , -0.25834212,  0.01129376, ..., -0.28837636,\n",
       "        -0.52628681,  0.1926041 ],\n",
       "       [ 1.        ,  0.68091731,  0.9475229 , ...,  0.30947023,\n",
       "         1.24967292, -0.76901449]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem 4\n",
    "\n",
    "Here, we are asked to essentially conduct ridge-regression. However, we compute this in a numerically stable and efficient method, using QR Decomposition. See Murphy 7.5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by adding \"pseudo-data\" to our matrix based on our prior distribution on weights. \n",
    "\n",
    "$$ \\bf{w} \\sim \\mathcal{N}(\\bf{0}, \\tau^2 \\bf{I})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (41167, 10)\n",
      "Y shape:  (41167, 1)\n"
     ]
    }
   ],
   "source": [
    "tau_inv_sq = 10.\n",
    "l = np.sqrt(10)\n",
    "\n",
    "# adding pseudo-data to X\n",
    "X = np.ones((D.shape[0] + D.shape[1], D.shape[1]))\n",
    "X[:D.shape[0],:] = D\n",
    "X[D.shape[0]:, :] = np.identity(D.shape[1])*l\n",
    "\n",
    "# adding pseudo-data to y\n",
    "y = np.zeros((D.shape[0]+D.shape[1],1))\n",
    "y[:D.shape[0], :] = Y_train[:, np.newaxis]\n",
    "\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"Y shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qr time:  0.05932116508483887\n"
     ]
    }
   ],
   "source": [
    "qr_t0 = time.time()\n",
    "# QR decomposition\n",
    "Q, R= np.linalg.qr(X, mode='reduced')\n",
    "Q1, R1 = linalg.qr(X, mode='economic')\n",
    "qr_tf = time.time()\n",
    "print(\"qr time: \", qr_tf - qr_t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.dot(R) - Q1.dot(R1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inversion and product time:  0.002474069595336914\n"
     ]
    }
   ],
   "source": [
    "# optimized way of finding an inverse\n",
    "inv_t0 = time.time()\n",
    "R_inv = linalg.solve_triangular(R, np.identity(R.shape[0]))\n",
    "A = np.dot(R_inv, Q.T)\n",
    "w = np.dot(A, y)\n",
    "inv_tf = time.time()\n",
    "print(\"inversion and product time: \", inv_tf - inv_t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (4573,10) and (101,1) not aligned: 10 (dim 1) != 101 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-cda6ea8dd7bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msq_e\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mRMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-67-cda6ea8dd7bf>\u001b[0m in \u001b[0;36mRMSE\u001b[0;34m(X_test, w)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mRMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0msq_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msq_e\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (4573,10) and (101,1) not aligned: 10 (dim 1) != 101 (dim 0)"
     ]
    }
   ],
   "source": [
    "def RMSE(X_test, w):\n",
    "    error = Y_test[:, np.newaxis] - X_test.dot(w)\n",
    "    sq_e = error**2\n",
    "    return np.sqrt(np.sum(sq_e)/Y_test.shape[0])\n",
    "\n",
    "RMSE(D_test,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.2098893712934"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_pred = D_test.dot(w)\n",
    "np.sqrt(mean_squared_error(Y_test[:, np.newaxis], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = Y_test[:, np.newaxis] - D_test.dot(w)\n",
    "sq_e = error.T.dot(error)\n",
    "np.sqrt(sq_e/Y_test.shape[0])\n",
    "Y_test.shape\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5\n",
    "\n",
    "We now try and compute this using PyTorch and the L-BFGS optimizer. The model is the same; we have some 10D feature set (including the bias term). We have assumed that our data was generated with additive Gaussian noise, and that we also have a prior probability distribution on our weights. \n",
    "\n",
    "Here, the L-BFGS function essentially optimizes our log-posterior function for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Full PyTorch treatment.\n",
    "\n",
    "Every matrix is constructed as PyTorch variable. Most important part is to consider the shape of the logloss.\n",
    "'''\n",
    "\n",
    "size = 10\n",
    "# construct a PyTorch variable array\n",
    "weights = Variable(torch.randn(10,1), requires_grad=True)\n",
    "#weights = Variable(torch.Tensor(10,1), requires_grad=True)\n",
    "#weights = Variable(torch.randn(10,1).type(Torch.DoubleTensor), requires_grad=True)\n",
    "\n",
    "# LBFGS optimizer\n",
    "optimizer = torch.optim.LBFGS([weights])\n",
    "\n",
    "X_torch = Variable(torch.Tensor(X), requires_grad=False)\n",
    "y_torch = Variable(torch.Tensor(y), requires_grad=False)\n",
    "\n",
    "for i in range(100):\n",
    "    def torch_black_box():\n",
    "        # setting gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # calculating the logloss\n",
    "        y_pred = X_torch.mm(weights)\n",
    "        v = y_torch.sub(y_pred)\n",
    "        v_T = torch.transpose(v, 0,1)\n",
    "        logloss = v_T.mm(v)\n",
    "        \n",
    "        #print('loss: ', logloss.data.numpy())\n",
    "        # calculating the gradients\n",
    "        logloss.backward()\n",
    "        \n",
    "        #print(logloss.size())\n",
    "        # returns logloss; needs the [0,0] argument because logloss is a [1,1] vector\n",
    "        return logloss[0,0]\n",
    "    optimizer.step(torch_black_box) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that if we just ask for weights, PyTorch lists only 4 digits after the decimal. However, this is a facet of just how it prints. We can force it to display everything by just converting to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 7.7415\n",
       " 5.5578\n",
       " 2.2519\n",
       " 1.0788\n",
       "-5.9118\n",
       "-1.7348\n",
       "-1.6388\n",
       "-0.2661\n",
       " 0.8178\n",
       "-0.6591\n",
       "[torch.FloatTensor of size 10x1]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.74153376],\n",
       "       [ 5.55781698],\n",
       "       [ 2.25191236],\n",
       "       [ 1.07879972],\n",
       "       [-5.91177702],\n",
       "       [-1.73480248],\n",
       "       [-1.63875651],\n",
       "       [-0.26610565],\n",
       "       [ 0.81781435],\n",
       "       [-0.65913391]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have already put in our information about $\\bf{w}$ into our data-set as pseudo-counts, the log-posterior distribution is simply the same as that of least-squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to manually enter the gradient, it is simply:$$\\partial_{\\bf{w}} = 2 \\bf{X^T X} \\bf{w} - 2\\bf{X^T}\\bf{y}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Manual PyTorch: remember that weights.grad has to be directly upgraded. logloss has to be a number\n",
    "'''\n",
    "\n",
    "weights = Variable(torch.randn(10), requires_grad=True)\n",
    "optimizer = torch.optim.LBFGS([weights])\n",
    "print(weights.size())\n",
    "for i in range(100):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        weights_data = weights.data.numpy()\n",
    "        \n",
    "        # computing Xw\n",
    "        Xw = X.dot(weights_data[:, np.newaxis])\n",
    "        \n",
    "        # computing (y - Xw).T (y-Xw)\n",
    "        v = y - Xw\n",
    "        logloss = v.T.dot(v)\n",
    "\n",
    "        # gradient\n",
    "        grad = X.T.dot(Xw) - X.T.dot(y)\n",
    "        # changing shape of gradient back to (10,)\n",
    "        g = grad[:,0]\n",
    "        \n",
    "        weights.grad = Variable(Tensor(g))\n",
    "        return logloss[0,0]\n",
    "    \n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.74153376,  5.5578208 ,  2.25190735,  1.07880127, -5.91177797,\n",
       "       -1.73480332, -1.63875473, -0.26610556,  0.81781411, -0.65913397], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we note that $\\boldsymbol{x} \\in \\mathbb{R}^m, m = 9$. In other words, our current feature-space consists of a 9-dim. vector. This is excluding the bias term.\n",
    "\n",
    "The required matrix $A$ is then $d \\times m$. $b$ is also $d$-dimensional. The affine-transformation is what we compute regression on. In other words, we have:\n",
    "$$y - \\boldsymbol{w^T}\\boldsymbol{cos(Ax + b)}$$\n",
    "\n",
    "If we want to write the entire computation out in matrix form, we note that the design matrix, $X$ is simply of dimension $N \\times d$, where $N$ is the number of data-points. $X$ looks like:\n",
    "$$X = \\begin{pmatrix}\n",
    "        \\cdots \\boldsymbol{cos(Ax^{(1)} + b)^T} \\cdots \\\\\n",
    "               \\vdots \\\\\n",
    "         \\cdots \\boldsymbol{cos(Ax^{(n)} + b)^T} \\cdots \\\\\n",
    "         \\end{pmatrix}$$\n",
    "         \n",
    "The final regression simply looks like:\n",
    "$$ \\boldsymbol{y} - \\boldsymbol{X w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QR\n",
    "With this form, we can can add the bias term (as an extra-column) and continue with our way of add-pseudocounts to calculate the MLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=100\n",
    "m=9\n",
    "\n",
    "# constructing affine-transformation\n",
    "A = np.random.normal(size=(m,d))\n",
    "b = np.random.uniform(low=0.0, high=2*np.pi, size=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing linear transformation for train data\n",
    "phi = np.ones((D.shape[0], d+1))\n",
    "X1 = D[:, :9]\n",
    "X1.shape\n",
    "phi[:,1:] = np.cos(X1.dot(A))\n",
    "\n",
    "# constructing transformation for test data\n",
    "phi_test = np.ones((D_test.shape[0], d+1))\n",
    "X2 = D_test[:, :9]\n",
    "X2.shape\n",
    "phi_test[:,1:] = np.cos(X2.dot(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41167, 10)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.54030231,  0.55928153,  0.34274388, ...,  0.99704781,\n",
       "         0.3445402 ,  0.93548348],\n",
       "       [ 0.54030231,  0.79042266, -0.00281772, ..., -0.89530206,\n",
       "         0.9858102 , -0.20439166],\n",
       "       [ 0.54030231,  0.79773131, -0.44238107, ...,  0.21150422,\n",
       "         0.6766587 ,  0.76697106],\n",
       "       ..., \n",
       "       [ 0.54030231,  0.97396412, -0.12281354, ...,  0.82433012,\n",
       "         0.65486552,  0.65869149],\n",
       "       [ 0.54030231,  0.40551315,  0.91963815, ..., -0.99749169,\n",
       "         0.8482995 , -0.94218078],\n",
       "       [ 0.54030231,  1.        ,  1.        , ...,  1.        ,\n",
       "         1.        ,  1.        ]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QR(Phi):\n",
    "    '''\n",
    "    Takes the input matrix and produces the weight vector\n",
    "    '''\n",
    "\n",
    "    tau_inv_sq = 10.\n",
    "    l = np.sqrt(10)\n",
    "\n",
    "    # adding pseudo-data to X\n",
    "    X = np.ones((Phi.shape[0] + Phi.shape[1], Phi.shape[1]))\n",
    "    X[:Phi.shape[0],:] = Phi\n",
    "    X[Phi.shape[0]:, :] = np.identity(Phi.shape[1])*l\n",
    "    \n",
    "    # adding pseudo-data to y\n",
    "    y = np.zeros((Phi.shape[0]+Phi.shape[1],1))\n",
    "    y[:Phi.shape[0], :] = Y_train[:, np.newaxis]\n",
    "\n",
    "    print(\"X shape: \", X.shape)\n",
    "    print(\"Y shape: \", y.shape)\n",
    "\n",
    "    qr_t0 = time.time()\n",
    "    # QR decomposition\n",
    "    Q, R= np.linalg.qr(X, mode='reduced')\n",
    "    inv_t0 = time.time()\n",
    "    R_inv = linalg.solve_triangular(R, np.identity(R.shape[0]))\n",
    "    A = np.dot(R_inv, Q.T)\n",
    "    w = np.dot(A, y)\n",
    "    inv_tf = time.time()\n",
    "    print(\"inversion and product time: \", inv_tf - inv_t0)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (41258, 101)\n",
      "Y shape:  (41258, 1)\n",
      "inversion and product time:  0.014184236526489258\n",
      "time 0.15530991554260254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(101, 1)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "w = QR(phi)\n",
    "tf = time.time()-t0\n",
    "print(\"time\", tf)\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.9274613131034215"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE(phi_test,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trial 1: RMSE - 4.9274613131034215; time: 0.14908885955810547"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Full PyTorch treatment.\n",
    "\n",
    "Every matrix is constructed as PyTorch variable. Most important part is to consider the shape of the logloss.\n",
    "'''\n",
    "t0_torch = time.time()\n",
    "# adding pseudo-data to X\n",
    "X = np.ones((phi.shape[0] + phi.shape[1], phi.shape[1]))\n",
    "X[:phi.shape[0],:] = phi\n",
    "X[phi.shape[0]:, :] = np.identity(phi.shape[1])*l\n",
    "    \n",
    "# adding pseudo-data to y\n",
    "y = np.zeros((phi.shape[0]+phi.shape[1],1))\n",
    "y[:phi.shape[0], :] = Y_train[:, np.newaxis]\n",
    "size = 10\n",
    "# construct a PyTorch variable array\n",
    "weights = Variable(torch.randn(d+1,1), requires_grad=True)\n",
    "#weights = Variable(torch.Tensor(10,1), requires_grad=True)\n",
    "#weights = Variable(torch.randn(10,1).type(Torch.DoubleTensor), requires_grad=True)\n",
    "\n",
    "# LBFGS optimizer\n",
    "optimizer = torch.optim.LBFGS([weights])\n",
    "\n",
    "X_torch = Variable(torch.Tensor(X), requires_grad=False)\n",
    "y_torch = Variable(torch.Tensor(y), requires_grad=False)\n",
    "\n",
    "for i in range(100):\n",
    "    def torch_black_box():\n",
    "        # setting gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # calculating the logloss\n",
    "        y_pred = X_torch.mm(weights)\n",
    "        v = y_torch.sub(y_pred)\n",
    "        v_T = torch.transpose(v, 0,1)\n",
    "        logloss = v_T.mm(v)\n",
    "        \n",
    "        #print('loss: ', logloss.data.numpy())\n",
    "        # calculating the gradients\n",
    "        logloss.backward()\n",
    "        \n",
    "        #print(logloss.size())\n",
    "        # returns logloss; needs the [0,0] argument because logloss is a [1,1] vector\n",
    "        return logloss[0,0]\n",
    "    optimizer.step(torch_black_box) \n",
    "tf_torch = time.time() - t0_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  1.020986795425415\n",
      "RMSE:  4.92746128426\n"
     ]
    }
   ],
   "source": [
    "r = RMSE(phi_test, weights.data.numpy())\n",
    "print(\"Time: \", tf_torch)\n",
    "print(\"RMSE: \", r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trial 1: RMSE: 4.92746128426; Time: 1.020986795425415"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
