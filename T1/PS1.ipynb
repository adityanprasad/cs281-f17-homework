{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preliminaries\n",
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading data using numpy\n",
    "data = np.loadtxt('CASP.csv', delimiter=',', skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45730, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = data[:, 0]\n",
    "X = data[:,1:]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating training data - 90;10 split\n",
    "l = int(.9*Y.shape[0])\n",
    "\n",
    "Y_train = Y[:l]\n",
    "Y_test = Y[l:]\n",
    "X_train = X[:l]\n",
    "X_test = X[l:]\n",
    "\n",
    "assert X_train.shape[0]==.9*X.shape[0]\n",
    "assert X_test.shape[0]==0.1*X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing features.\n",
    "\n",
    "For each feature, $X_i$, we have $X_i = \\sigma  Z + \\mu \\implies Z =  \\frac{(X_i - \\mu)}{\\sigma}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41157, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalizing test data across each axis. This returns a vector with the mean of each column\n",
    "mu_train = X_train.mean(axis=0)\n",
    "var_train = X_train.var(axis=0)\n",
    "\n",
    "mu_test = X_test.mean(axis=0)\n",
    "var_test = X_test.var(axis=0)\n",
    "\n",
    "#print(mu_x.shape)\n",
    "\n",
    "# renaming normalized variables; there is numpy broadcasting magic going on here: mu_x has dim (9,) and X_train\n",
    "# has dim (41157,9) => X_train - mu_x will have dim (41157,9)\n",
    "Z = (X_train - mu_train)/np.sqrt(var_train)\n",
    "Z_test = (X_test - mu_test)/np.sqrt(var_test)\n",
    "Z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding bias feature (to first column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = np.ones((Z.shape[0], Z.shape[1]+1))\n",
    "D[:, 1:] = Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bias for test data\n",
    "D_test = np.ones((Z_test.shape[0], Z_test.shape[1]+1))\n",
    "D_test[:, 1:] = Z_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.39767095,  0.64721867, ..., -0.64707638,\n",
       "        -0.10005648,  0.7050291 ],\n",
       "       [ 1.        ,  0.29926327, -0.0535137 , ..., -0.08606622,\n",
       "        -0.98803634,  0.02224952],\n",
       "       [ 1.        ,  0.09910086, -0.38016704, ..., -0.11615054,\n",
       "        -0.5795656 , -0.22750538],\n",
       "       ..., \n",
       "       [ 1.        , -0.53920322, -0.37045806, ..., -0.35353095,\n",
       "        -0.43748883,  0.5025857 ],\n",
       "       [ 1.        , -0.25834212,  0.01129376, ..., -0.28837636,\n",
       "        -0.52628681,  0.1926041 ],\n",
       "       [ 1.        ,  0.68091731,  0.9475229 , ...,  0.30947023,\n",
       "         1.24967292, -0.76901449]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem 4\n",
    "\n",
    "Here, we are asked to essentially conduct ridge-regression. However, we compute this in a numerically stable and efficient method, using QR Decomposition. See Murphy 7.5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by adding \"pseudo-data\" to our matrix based on our prior distribution on weights. \n",
    "\n",
    "$$ \\bf{w} \\sim \\mathcal{N}(\\bf{0}, \\tau^2 \\bf{I})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (41167, 10)\n",
      "Y shape:  (41167, 1)\n"
     ]
    }
   ],
   "source": [
    "tau_inv_sq = 10.\n",
    "l = np.sqrt(10)\n",
    "\n",
    "# adding pseudo-data to X\n",
    "X = np.ones((D.shape[0] + D.shape[1], D.shape[1]))\n",
    "X[:D.shape[0],:] = D\n",
    "X[D.shape[0]:, :] = np.identity(D.shape[1])*l\n",
    "\n",
    "# adding pseudo-data to y\n",
    "y = np.zeros((D.shape[0]+D.shape[1],1))\n",
    "y[:D.shape[0], :] = Y_train[:, np.newaxis]\n",
    "\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"Y shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qr time:  0.023701906204223633\n"
     ]
    }
   ],
   "source": [
    "qr_t0 = time.time()\n",
    "# QR decomposition\n",
    "Q, R= np.linalg.qr(X, mode='reduced')\n",
    "Q1, R1 = linalg.qr(X, mode='economic')\n",
    "qr_tf = time.time()\n",
    "print(\"qr time: \", qr_tf - qr_t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.dot(R) - Q1.dot(R1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inversion and product time:  0.0016148090362548828\n",
      "[[ 7.74153395]\n",
      " [ 5.55782079]\n",
      " [ 2.25190765]\n",
      " [ 1.07880135]\n",
      " [-5.91177796]\n",
      " [-1.73480336]\n",
      " [-1.63875478]\n",
      " [-0.26610556]\n",
      " [ 0.81781409]\n",
      " [-0.65913397]]\n"
     ]
    }
   ],
   "source": [
    "# optimized way of finding an inverse\n",
    "inv_t0 = time.time()\n",
    "R_inv = linalg.solve_triangular(R, np.identity(R.shape[0]))\n",
    "A = np.dot(R_inv, Q.T)\n",
    "w = np.dot(A, y)\n",
    "inv_tf = time.time()\n",
    "print(\"inversion and product time: \", inv_tf - inv_t0)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.2098893712934"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def RMSE(X_test, w):\n",
    "    error = Y_test[:, np.newaxis] - X_test.dot(w)\n",
    "    sq_e = error**2\n",
    "    return np.sqrt(np.sum(sq_e)/Y_test.shape[0])\n",
    "\n",
    "RMSE(D_test,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.2098893712934"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_pred = D_test.dot(w)\n",
    "np.sqrt(mean_squared_error(Y_test[:, np.newaxis], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = Y_test[:, np.newaxis] - D_test.dot(w)\n",
    "sq_e = error.T.dot(error)\n",
    "np.sqrt(sq_e/Y_test.shape[0])\n",
    "Y_test.shape\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5\n",
    "\n",
    "We now try and compute this using PyTorch and the L-BFGS optimizer. The model is the same; we have some 10D feature set (including the bias term). We have assumed that our data was generated with additive Gaussian noise, and that we also have a prior probability distribution on our weights. \n",
    "\n",
    "Here, the L-BFGS function essentially optimizes our log-posterior function for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Full PyTorch treatment.\n",
    "\n",
    "Every matrix is constructed as PyTorch variable. Most important part is to consider the shape of the logloss.\n",
    "'''\n",
    "\n",
    "size = 10\n",
    "# construct a PyTorch variable array\n",
    "weights = Variable(torch.randn(10,1), requires_grad=True)\n",
    "#weights = Variable(torch.Tensor(10,1), requires_grad=True)\n",
    "#weights = Variable(torch.randn(10,1).type(Torch.DoubleTensor), requires_grad=True)\n",
    "\n",
    "# LBFGS optimizer\n",
    "optimizer = torch.optim.LBFGS([weights])\n",
    "\n",
    "X_torch = Variable(torch.Tensor(X), requires_grad=False)\n",
    "y_torch = Variable(torch.Tensor(y), requires_grad=False)\n",
    "\n",
    "for i in range(100):\n",
    "    def torch_black_box():\n",
    "        # setting gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # calculating the logloss\n",
    "        y_pred = X_torch.mm(weights)\n",
    "        v = y_torch.sub(y_pred)\n",
    "        v_T = torch.transpose(v, 0,1)\n",
    "        logloss = v_T.mm(v)\n",
    "        \n",
    "        #print('loss: ', logloss.data.numpy())\n",
    "        # calculating the gradients\n",
    "        logloss.backward()\n",
    "        \n",
    "        #print(logloss.size())\n",
    "        # returns logloss; needs the [0,0] argument because logloss is a [1,1] vector\n",
    "        return logloss[0,0]\n",
    "    optimizer.step(torch_black_box) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that if we just ask for weights, PyTorch lists only 4 digits after the decimal. However, this is a facet of just how it prints. We can force it to display everything by just converting to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 7.7415\n",
       " 5.5578\n",
       " 2.2519\n",
       " 1.0788\n",
       "-5.9118\n",
       "-1.7348\n",
       "-1.6388\n",
       "-0.2661\n",
       " 0.8178\n",
       "-0.6591\n",
       "[torch.FloatTensor of size 10x1]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.2098893822576393"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE(D_test, weights.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have already put in our information about $\\bf{w}$ into our data-set as pseudo-counts, the log-posterior distribution is simply the same as that of least-squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to manually enter the gradient, it is simply:$$\\partial_{\\bf{w}} = 2 \\bf{X^T X} \\bf{w} - 2\\bf{X^T}\\bf{y}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Manual PyTorch: remember that weights.grad has to be directly upgraded. logloss has to be a number\n",
    "'''\n",
    "\n",
    "weights = Variable(torch.randn(10), requires_grad=True)\n",
    "optimizer = torch.optim.LBFGS([weights])\n",
    "print(weights.size())\n",
    "for i in range(100):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        weights_data = weights.data.numpy()\n",
    "        \n",
    "        # computing Xw\n",
    "        Xw = X.dot(weights_data[:, np.newaxis])\n",
    "        \n",
    "        # computing (y - Xw).T (y-Xw)\n",
    "        v = y - Xw\n",
    "        logloss = v.T.dot(v)\n",
    "\n",
    "        # gradient\n",
    "        grad = X.T.dot(Xw) - X.T.dot(y)\n",
    "        # changing shape of gradient back to (10,)\n",
    "        g = grad[:,0]\n",
    "        \n",
    "        weights.grad = Variable(Tensor(g))\n",
    "        return logloss[0,0]\n",
    "    \n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468.12642247904188"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.data.numpy()\n",
    "RMSE(D_test, weights.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we note that $\\boldsymbol{x} \\in \\mathbb{R}^m, m = 9$. In other words, our current feature-space consists of a 9-dim. vector. This is excluding the bias term.\n",
    "\n",
    "The required matrix $A$ is then $d \\times m$. $b$ is also $d$-dimensional. The affine-transformation is what we compute regression on. In other words, we have:\n",
    "$$y - \\boldsymbol{w^T}\\boldsymbol{cos(Ax + b)}$$\n",
    "\n",
    "If we want to write the entire computation out in matrix form, we note that the design matrix, $X$ is simply of dimension $N \\times d$, where $N$ is the number of data-points. $X$ looks like:\n",
    "$$X = \\begin{pmatrix}\n",
    "        \\cdots \\boldsymbol{cos(Ax^{(1)} + b)^T} \\cdots \\\\\n",
    "               \\vdots \\\\\n",
    "         \\cdots \\boldsymbol{cos(Ax^{(n)} + b)^T} \\cdots \\\\\n",
    "         \\end{pmatrix}$$\n",
    "         \n",
    "The final regression simply looks like:\n",
    "$$ \\boldsymbol{y} - \\boldsymbol{X w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QR\n",
    "With this form, we can can add the bias term (as an extra-column) and continue with our way of add-pseudocounts to calculate the MLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d=800\n",
    "m=9\n",
    "\n",
    "# constructing affine-transformation\n",
    "A = np.random.normal(size=(m,d))\n",
    "b = np.random.uniform(low=0.0, high=2*np.pi, size=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# constructing linear transformation for train data\n",
    "phi = np.ones((D.shape[0], d+1))\n",
    "X1 = D[:, :9]\n",
    "X1.shape\n",
    "phi[:,1:] = np.cos(X1.dot(A))\n",
    "\n",
    "# constructing transformation for test data\n",
    "phi_test = np.ones((D_test.shape[0], d+1))\n",
    "X2 = D_test[:, :9]\n",
    "X2.shape\n",
    "phi_test[:,1:] = np.cos(X2.dot(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41167, 10)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.83618849,  0.43406386, ...,  0.95214493,\n",
       "        -0.09399051,  0.58364955],\n",
       "       [ 1.        ,  0.65514863, -0.99494424, ..., -0.94712767,\n",
       "         0.4750537 ,  0.45211841],\n",
       "       [ 1.        ,  0.98451674, -0.9837253 , ..., -0.24830389,\n",
       "         0.67539518, -0.02053922],\n",
       "       ..., \n",
       "       [ 1.        , -0.66539801,  0.03379376, ..., -0.82546774,\n",
       "         0.20784028,  0.96599343],\n",
       "       [ 1.        , -0.73328698,  0.26283038, ...,  0.85507212,\n",
       "        -0.26826267,  0.0988909 ],\n",
       "       [ 1.        , -0.87217987, -0.81976185, ..., -0.93546348,\n",
       "         0.03570028,  0.95653922]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def QR(Phi):\n",
    "    '''\n",
    "    Takes the input matrix and produces the weight vector\n",
    "    '''\n",
    "\n",
    "    tau_inv_sq = 10.\n",
    "    l = np.sqrt(10)\n",
    "\n",
    "    # adding pseudo-data to X\n",
    "    X = np.ones((Phi.shape[0] + Phi.shape[1], Phi.shape[1]))\n",
    "    X[:Phi.shape[0],:] = Phi\n",
    "    X[Phi.shape[0]:, :] = np.identity(Phi.shape[1])*l\n",
    "    \n",
    "    # adding pseudo-data to y\n",
    "    y = np.zeros((Phi.shape[0]+Phi.shape[1],1))\n",
    "    y[:Phi.shape[0], :] = Y_train[:, np.newaxis]\n",
    "\n",
    "    print(\"X shape: \", X.shape)\n",
    "    print(\"Y shape: \", y.shape)\n",
    "\n",
    "    qr_t0 = time.time()\n",
    "    # QR decomposition\n",
    "    Q, R= np.linalg.qr(X, mode='reduced')\n",
    "    inv_t0 = time.time()\n",
    "    R_inv = linalg.solve_triangular(R, np.identity(R.shape[0]))\n",
    "    A = np.dot(R_inv, Q.T)\n",
    "    w = np.dot(A, y)\n",
    "    inv_tf = time.time()\n",
    "    print(\"inversion and product time: \", inv_tf - inv_t0)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (41958, 801)\n",
      "Y shape:  (41958, 1)\n",
      "inversion and product time:  0.6742129325866699\n",
      "time 5.143468141555786\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(801, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "w = QR(phi)\n",
    "tf = time.time()-t0\n",
    "print(\"time\", tf)\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.415891921337316"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE(phi_test,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d = 100\n",
    "1. Trial 1: RMSE - 4.9274613131034215; time: 0.14908885955810547\n",
    "2. Trial 2: RMSE - 5.0312886993334871; time: 0.3584740161895752\n",
    "3. Trial 3: RMSE - 4.8544483102786247; time: 0.211525917053222\n",
    "\n",
    "d = 200\n",
    "1. RMSE: 4.7517805676202389; time: 0.527772903442382\n",
    "2. 4.7224569427197922; time 0.4477860927581787\n",
    "3. 4.7383303898457196; 0.4993929862976074\n",
    "\n",
    "d = 400\n",
    "1. 4.536067377039938; 1.7231628894805908\n",
    "2. 4.5549660329440762; 2.02138614654541\n",
    "3. 4.5630712841943097; 1.285445213317871\n",
    "\n",
    "d = 800\n",
    "1. 4.4547643426722745; 4.23047399520874 \n",
    "2. 4.4506298382389291; time 4.3787009716033936\n",
    "3. 4.4374408003731141; time 4.27150821685791"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.9377327742385111"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average([4.9274613131034215, 5.0312886993334871,4.8544483102786247 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.7375226333952503"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average([4.7517805676202389,4.7224569427197922,  4.7383303898457196])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5513682313927744"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average([4.536067377039938,4.5549660329440762, 4.5630712841943097 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.4476116604281062"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average([4.4547643426722745, 4.4506298382389291, 4.4374408003731141])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Full PyTorch treatment.\n",
    "\n",
    "Every matrix is constructed as PyTorch variable. Most important part is to consider the shape of the logloss.\n",
    "'''\n",
    "t0_torch = time.time()\n",
    "# adding pseudo-data to X\n",
    "X = np.ones((phi.shape[0] + phi.shape[1], phi.shape[1]))\n",
    "X[:phi.shape[0],:] = phi\n",
    "X[phi.shape[0]:, :] = np.identity(phi.shape[1])*l\n",
    "    \n",
    "# adding pseudo-data to y\n",
    "y = np.zeros((phi.shape[0]+phi.shape[1],1))\n",
    "y[:phi.shape[0], :] = Y_train[:, np.newaxis]\n",
    "size = 10\n",
    "# construct a PyTorch variable array\n",
    "weights = Variable(torch.randn(d+1,1), requires_grad=True)\n",
    "#weights = Variable(torch.Tensor(10,1), requires_grad=True)\n",
    "#weights = Variable(torch.randn(10,1).type(Torch.DoubleTensor), requires_grad=True)\n",
    "\n",
    "# LBFGS optimizer\n",
    "optimizer = torch.optim.LBFGS([weights])\n",
    "\n",
    "X_torch = Variable(torch.Tensor(X), requires_grad=False)\n",
    "y_torch = Variable(torch.Tensor(y), requires_grad=False)\n",
    "\n",
    "for i in range(100):\n",
    "    def torch_black_box():\n",
    "        # setting gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # calculating the logloss\n",
    "        y_pred = X_torch.mm(weights)\n",
    "        v = y_torch.sub(y_pred)\n",
    "        v_T = torch.transpose(v, 0,1)\n",
    "        logloss = v_T.mm(v)\n",
    "        \n",
    "        #print('loss: ', logloss.data.numpy())\n",
    "        # calculating the gradients\n",
    "        logloss.backward()\n",
    "        \n",
    "        #print(logloss.size())\n",
    "        # returns logloss; needs the [0,0] argument because logloss is a [1,1] vector\n",
    "        return logloss[0,0]\n",
    "    optimizer.step(torch_black_box) \n",
    "tf_torch = time.time() - t0_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  15.923546075820923\n",
      "RMSE:  4.41590726585\n"
     ]
    }
   ],
   "source": [
    "r = RMSE(phi_test, weights.data.numpy())\n",
    "print(\"Time: \", tf_torch)\n",
    "print(\"RMSE: \", r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d=100\n",
    "1. Trial 1: RMSE: 4.92746128426; Time: 1.020986795425415\n",
    "2. Trial 2: RMSE: 5.03128867109; Time: 1.160470962524414\n",
    "3. Trial 3: RMSE: 4.85444859004; Time: 1.0120360851287842\n",
    "\n",
    "d=200\n",
    "1. Time:  1.0120360851287842; RMSE:  4.85444859004\n",
    "2. Time:  2.025758981704712; RMSE:  4.72245696161\n",
    "3. 0.4993929862976074; 2.143263101577759\n",
    "\n",
    "d=400\n",
    "1. Time:  4.634213924407959; RMSE:  4.53606708706\n",
    "2. Time:  4.8693249225616455; RMSE:  4.55496555986\n",
    "3. Time:  4.485335111618042; RMSE:  4.56307137668\n",
    "\n",
    "d=800\n",
    "1. Time:  11.636758089065552; RMSE:  4.45467260382\n",
    "2. Time:  12.416048288345337; RMSE:  4.45063399708\n",
    "3. Time:  11.092233180999756; RMSE:  4.43740281478"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0644979476928711"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d = 100\n",
    "np.average([4.92746128426, 5.03128867109, 4.85444859004])\n",
    "np.average([1.020986795425415, 1.160470962524414, 1.0120360851287842])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.7884527758250002"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d = 200 \n",
    "np.average([4.85444859004, 4.72245696161])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5513680078666665"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d=400\n",
    "np.average([4.53606708706, 4.55496555986, 4.56307137668])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.4475698052266663"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d=800\n",
    "np.average([4.45467260382,4.45063399708, 4.43740281478 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFP5JREFUeJzt3X+Q3HWd5/HnO0NiskKACnMLMiwTfohkgQwycg4RbmCO\nBSQLHue5sJvdkjrkghthUUAD3JV36mU17sJ63lUKo7elORe3/IEct7u6lTiEwqnDCZlEYxaDBLwh\nWBkjXg4wBJL3/dGdr5MfMxMm0/2dnn4+qlL97f5++tvv/qa7X/P9fL4/IjORJAlgWtkFSJImD0NB\nklQwFCRJBUNBklQwFCRJBUNBklQwFCRJBUNBklQwFCRJhaPKLuCNOuGEE7K9vb3sMiSpoaxbt+4X\nmdk6VruGC4X29nb6+/vLLkOSGkpEPHc47ew+kiQVDAVJUsFQkCQVGm5MQZIO9NprrzE4OMiuXbvK\nLqV0M2fOpK2tjenTp4/r+YaCpIY3ODjIMcccQ3t7OxFRdjmlyUx27NjB4OAgc+fOHdcy7D6S1PB2\n7drFnDlzmjoQACKCOXPmHNEWU/OEQl8fLFtWuZ3imuitSoVmD4R9jnQ9NEf3UV8f9PTA7t0wYwas\nXg1dXWVXVRNN9FYl1UBzbCn09lZ+Jffsqdz29pZdUc000VuVJpWjjz76oMc+/vGPc/LJJ9PR0cHb\n3vY2brnlFvbu3QvA+9//fubOnUtHRwcdHR187nOfA+Cll17illtu4fTTT+ftb387F1xwAV/4whcA\n2Lt3L7feeivnnHMO5557Lu94xzvYunXrhL6P5thS6O6u/Nm878/n7u6yK6qZJnqrUkO4/fbbueOO\nO9i7dy+XXHIJjz76KJdeeikAy5cv573vfe9+7W+66SZOO+00tmzZwrRp0xgaGuJLX/oSAF/72tfY\ntm0bGzduZNq0aQwODvLmN795QuttjlDo6qr0o/T2Vn4lp3B/ShO9VemI9PXV93uye/dudu3axfHH\nHz9im5/+9Kc88cQTfPWrX2XatEpHTmtrKx/96EcBeOGFFzjppJOKeW1tbRNeZ3OEAlT+15vkF7KJ\n3qo0LvUce7vvvvtYtWoVzz33HFdddRUdHR3FvDvvvJNPfvKTAHzlK19h69atzJ8/v/jRP9D73vc+\n3vWud/HYY4/R09PDokWLOP/88ye03uYYU5CkYeo59nb77bczMDDA9u3befnll3nwwQeLecuXL2dg\nYICBgQHOPffcg577qU99io6ODt7ylrcAlS2Dp556imXLljFt2jR6enpYvXr1hNZrKEhqOvvG3lpa\n6jf2Nn36dK688krWrl07Ypt58+axYcOGYjD6nnvuYWBggJ07dxZt3vSmN3HVVVexfPly7r77bh56\n6KEJrdNQkNR09o29feIT9dttOzN5/PHHOf3000dsc8YZZ9DZ2cm9997Lnj17gMqBeZkJwJNPPsm2\nbduAyp5IGzdu5NRTT53QOptnTEGShpnosbdXXnllv4HfD3/4w8BvxhRee+01zjvvPD74wQ+OupyV\nK1dy5513csYZZzBnzhxmzZrFZz7zGQC2b9/OBz7wAV599VUALrzwQpYsWTJxbwKIfQnUKDo7O9OL\n7EgabvPmzZx99tlllzFpHGp9RMS6zOwc67l2H0mSCoaCJKlgKEiSCjUPhYhoiYj1EfHIIeYdHxHf\nioiNEfFERJxT63okSSOrx5bCbcDmEebdDQxk5nnAnwB/VYd6JEkjqGkoREQbcDWwcoQm84A1AJn5\nT0B7RPx2LWuSJI2s1lsK9wN3AXtHmL8BuA4gIi4ETgUOOsNTRNwcEf0R0T80NFSrWiVp3AYHB7n2\n2ms588wzOe2001iyZAmvvvoqvb29HHvsscXps++4446ySx1VzUIhIhYC2zNz3SjN/hw4LiIGgA8B\n64E9BzbKzAcyszMzO1tbW2tTsCSNU2Zy3XXX8Z73vIctW7awZcsWfv3rX3PXXXcBcPHFFzMwMMD6\n9et55JFHePzxx0uueGS1PKJ5AXBNRLwbmAnMjohVmbloX4PM3AncCBCVa8htBZ6pYU2SVDGB585e\ns2YNM2fO5MYbbwSgpaWF++67j1NPPZXLL7+8aDdr1iw6Ojp4/vnnj+j1aqlmoZCZS4GlABHRDdwx\nPBCqjx8HvJKZu4GbgLXVoJCk2pngc2dv2rSJCy64YL/HZs+eTXt7O08//XTx2IsvvsiWLVu45JJL\nxv1atVb34xQiYnFELK7ePRv4UUQ8BVxFZU8lSaqtOl+39rHHHmP+/PmcfPLJXHHFFZx44ok1fb0j\nUZdQyMzezFxYnV6RmSuq032Z+dbMPCszr8vMF+tRj6QmN8Hnzp43bx7r1u0/fLpz505+/vOfc9ZZ\nZ3HxxRezYcMGNm3axBe/+EUGBgaO6PVqySOaJTWfCT53dk9PD6+88gpf/vKXAdizZw8f+chHWLJk\nCbNmzSrazZ07l4997GN8+tOfPqLXqyVDQVJz6uqCpUsn5PzZEcG3vvUtvv71r3PmmWcyZ84cpk2b\nxj333HNQ28WLF7N27VqeffbZI37dWvB6CpI0AU455RQefvhhAL7//e9zww038OSTT9Ld3U33sO6p\nWbNmNefeR5LUrC666CKee+65sssYF7uPJEkFQ0HSlNBoV5GslSNdD4aCpIY3c+ZMduzY0fTBkJns\n2LGDmTNnjnsZjilIanhtbW0MDg7iCTMrAdnWdtB5RQ+boSCp4U2fPp25c+eWXcaUYPeRJKlgKEiS\nCoaCJKlgKEiSCoaCJKlgKEiSCoaCJKlgKEiSCoaCJKlgKEiSCoaCJKlgKEiSCoaCJKlgKEiSCoaC\nJKlgKEiSCoaCJKlgKEiSCoaCJKlgKEiSCoaCJKlgKEiSCjUPhYhoiYj1EfHIIeYdGxH/MyI2RMSm\niLix1vVIkkZWjy2F24DNI8z7U+DHmTkf6Ab+IiJm1KEmSdIh1DQUIqINuBpYOUKTBI6JiACOBn4J\nvF7LmiRJIzuqxsu/H7gLOGaE+Z8HHga2Vdv8QWburXFNkqQR1GxLISIWAtszc90oza4ABoC3AB3A\n5yNi9iGWdXNE9EdE/9DQUG0KliTVtPtoAXBNRDwLPAhcFhGrDmhzI/DNrHga2Aq87cAFZeYDmdmZ\nmZ2tra01LFmSmlvNQiEzl2ZmW2a2A9cDazJz0QHNfgb0AETEbwNnAc/UqiY1l74+WLascivp8NR6\nTOEgEbEYIDNXAJ8A/joifggE8NHM/EW9a9LU09cHPT2wezfMmAGrV0NXV9lVSZNfXUIhM3uB3ur0\nimGPbwN+rx41qLn09lYCYc+eym1vr6EgHQ6PaNaU1N1d2UJoaancdneXXZHUGOrefSTVQ1dXpcuo\nt7cSCG4lSIfHUNCU1dVlGEhvlN1HkqSCoSBJKhgKkqSCoSBJKhgKkqSCoSBJKhgKkqSCoSBJKhgK\nkqSCoSBJKhgKkqSCoSBJKhgKkqSCoSBJKhgKkqSCoTBeXhVe0hTkRXbGw6vCS5qi3FIYj0NdFV6S\npgBDYTy8KrykKcruo/HwqvCSpihDYby8KrykKcjuI0lSwVCQJBVGDYWIuGzY9NwD5l1Xq6IkSeUY\na0vhs8Omv3HAvHsnuBZJUsnGCoUYYfpQ9yVJDW6sUMgRpg91X5LU4MbaJfW0iHiYylbBvmmq9+eO\n/DRJUiMaKxSuHTb92QPmHXj/kCKiBegHns/MhQfMuxP4o2G1nA20ZuYvD2fZkqSJNWooZOajw+9H\nxHTgHCo/8NsP8zVuAzYDsw+x/OXA8uqyfx+43UCQpPKMtUvqioj43er0scAG4MvA+oi4YayFR0Qb\ncDWw8jBquQH4m8NoJ0mqkbEGmi/OzE3V6RuBn2TmucAFwF2Hsfz7q+32jtYoIn4LuJKDd3uVJNXR\nWKGwe9j05cBDAJn587EWHBELge2Zue4w6vh94PGRuo4i4uaI6I+I/qGhocNYnCRpPMYKhV9FxMKI\nOB9YAPwDQEQcBcwa47kLgGsi4lngQeCyiFg1QtvrGaXrKDMfyMzOzOxsbW0d42UlSeM1Vij8O2AJ\n8N+BPxu2hdAD/K/RnpiZSzOzLTPbqfzor8nMRQe2q45V/Avg22+wdknSBBtr76OfUOnrP/Dx7wDf\nGc8LRsTi6jJWVB/6V8B3M/Pl8SxPkjRxInPkA5Mj4nOjPTkzb53wisbQ2dmZ/f399X5ZSWpoEbEu\nMzvHajfWwWuLgR8Bfwtsw/MdSdKUNlYonAT8G+APgNeBrwFfz8xf1bowSVL9jTrQnJk7MnNFZl5K\n5TiF44AfR8Qf16U6SVJdHdY1miPi7VSOOL4c+HvgcI49kCQ1mFFDISL+E5XTVGymcqzB0sx8vR6F\nSZLqb6wthXuBrcD86r//HBFQGXDOzDyvtuVJkupprFDwmgmS1ETGOnjtuUM9HhHTqIwxHHK+JKkx\njXXq7NkRsTQiPh8RvxcVHwKeAd5XnxIlSfUyVvfRV4AXgT7gJuBuKuMJ78nMgRrXJkmqszGv0Vy9\nfgIRsRJ4AfidzNxV88okSXU31llSX9s3kZl7gEEDQZKmrrG2FOZHxM7qdACzqvf37ZJ60HWXJUmN\na6y9j1rqVYgkqXxjdR9JkpqIoSBJKhgKkqSCoSBJKhgKkqSCoSBJKhgKkqSCoSBJKhgKkqSCoSBJ\nKhgKkqSCoSBJKhgKkqSCoSBJKhgKkqSCoSBJKhgKkqSCoSBJKtQ8FCKiJSLWR8QjI8zvjoiBiNgU\nEY/Wuh5J0shGvUbzBLkN2AzMPnBGRBwH/Dfgysz8WUT8szrUI0kaQU23FCKiDbgaWDlCkz8EvpmZ\nPwPIzO21rEeSNLpadx/dD9wF7B1h/luB4yOiNyLWRcSfHKpRRNwcEf0R0T80NFSrWiWp6dUsFCJi\nIbA9M9eN0uwo4AIqWxNXAP8+It56YKPMfCAzOzOzs7W1tTYFS5JqOqawALgmIt4NzARmR8SqzFw0\nrM0gsCMzXwZejoi1wHzgJzWsS5I0gpptKWTm0sxsy8x24HpgzQGBAPBt4F0RcVRE/Bbwz6kMSkuS\nSlCPvY/2ExGLATJzRWZujoh/ADZSGXdYmZk/qndNkqSKyMyya3hDOjs7s7+/v+wyJKmhRMS6zOwc\nq51HNEuSCoaCJKlgKEiSCoaCJKlgKEiSCoaCJKlgKEiSCoaCJKlgKEiSCoaCJKlgKEiSCoaCJKlg\nKGhy6OuDZcsqt5IOUq+vSN1PnS0dpK8Penpg926YMQNWr4aurrKrkiaNen5F3FJQ+Xp7K5/2PXsq\nt729ZVckTSr1/IoYCipfd3flz5+Wlsptd3fZFUmTSj2/InYfqXxdXZXt4d7eyqfdriNpP/X8injl\nNUlqAl55TZL0hhkKkqSCoSBJKhgKkqSCoSBJKhgKkqSCoSBJKhgKkqSCoSBJKhgKkqSCoSBJKhgK\nkqSCoSBJKtQ8FCKiJSLWR8Qjh5jXHRH/NyIGqv/+Q63rkSSNrB7XU7gN2AzMHmH+Y5m5sA51SJLG\nUNMthYhoA64GVtbydSRJE6PW3Uf3A3cBe0dpc1FEbIyIv4+I361xPZKkUdQsFCJiIbA9M9eN0uxJ\n4Hcy8zzgvwAPjbCsmyOiPyL6h4aGalCtJAlqu6WwALgmIp4FHgQui4hVwxtk5s7MfKk6/XfA9Ig4\n4cAFZeYDmdmZmZ2tra01LFmSmlvNQiEzl2ZmW2a2A9cDazJz0fA2EXFiRER1+sJqPTtqVZMkaXT1\n2PtoPxGxGCAzVwDvBW6JiNeBXwPXZ2bWuyZJUkU02m9wZ2dn9vf3l12GJDWUiFiXmZ1jtfOIZklS\nwVCQJBUMBUlSwVCQJBUMBUlSwVCQJBUMBUlSwVCQJBUMBUlSwVCQJBUMBUlSwVCQJBUMBUlSwVCQ\nJBUMBdVdXx8sW1a5laaEKfShrvtFdtTc+vqgpwd274YZM2D1aujqKrsq6QhMsQ+1Wwqqq97eyndn\nz57KbW9v2RVJR2iKfagNBdVVd3flj6mWlsptd3fZFUlHaIp9qO0+Ul11dVW2rnt7K9+dBt7Kliqm\n2IfaazRLUhPwGs2SpDfMUJAkFQwFSVLBUJAkFQwFSVLBUJAkFRpul9SIGAJeBn5Rdi2TzAm4ToZz\nfRzMdXKwZlonp2Zm61iNGi4UACKi/3D2t20mrpP9uT4O5jo5mOvkYHYfSZIKhoIkqdCoofBA2QVM\nQq6T/bk+DuY6OZjr5AANOaYgSaqNRt1SkCTVQEOFQkRcGRFPRcTTEfGxsuuZDCLiSxGxPSJ+VHYt\nk0FEnBIR34uIH0fEpoi4reyayhYRMyPiiYjYUF0n/7HsmiaDiGiJiPUR8UjZtUwmDRMKEdEC/Ffg\nKmAecENEzCu3qknhr4Eryy5iEnkd+EhmzgPeCfypnxNeBS7LzPlAB3BlRLyz5Jomg9uAzWUXMdk0\nTCgAFwJPZ+YzmbkbeBC4tuSaSpeZa4Ffll3HZJGZL2Tmk9Xp/0flS39yuVWVKyteqt6dXv3X1IOJ\nEdEGXA2sLLuWyaaRQuFk4P8Muz9Ik3/ZNbqIaAfOB/53uZWUr9pVMgBsB/4xM5t9ndwP3AXsLbuQ\nyaaRQkE6bBFxNPAN4M8yc2fZ9ZQtM/dkZgfQBlwYEeeUXVNZImIhsD0z15Vdy2TUSKHwPHDKsPtt\n1cek/UTEdCqB8D8y85tl1zOZZOavgO/R3ONQC4BrIuJZKt3Ql0XEqnJLmjwaKRR+AJwZEXMjYgZw\nPfBwyTVpkomIAL4IbM7Mvyy7nskgIloj4rjq9CzgcuCfyq2qPJm5NDPbMrOdyu/ImsxcVHJZk0bD\nhEJmvg4sAb5DZfDwbzNzU7lVlS8i/gboA86KiMGI+Ldl11SyBcAfU/nrb6D6791lF1Wyk4DvRcRG\nKn9c/WNmuhumDskjmiVJhYbZUpAk1Z6hIEkqGAqSpIKhIEkqGAqSpMJRZRcgTVYRMQdYXb17IrAH\nGKrefyUzLyqlMKmG3CVVOgwR8XHgpcz8bNm1SLVk95E0DhHxUvW2OyIejYhvR8QzEfHnEfFH1esX\n/DAiTq+2a42Ib0TED6r/FpT7DqRDMxSkIzcfWAycTeVo6rdm5oVUTsv8oWqbvwLuy8x3AP8aT9ms\nScoxBenI/SAzXwCIiJ8C360+/kPg0ur0vwTmVU7NBMDsiDh62HUOpEnBUJCO3KvDpvcOu7+X33zH\npgHvzMxd9SxMeqPsPpLq47v8piuJiOgosRZpRIaCVB+3Ap0RsTEifkxlDEKadNwlVZJUcEtBklQw\nFCRJBUNBklQwFCRJBUNBklQwFCRJBUNBklQwFCRJhf8Pc9gsvyksLkUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x138471f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rmse1 = [4.9377328484633338,4.7884527758250002,4.5513680078666665,  4.4475698052266663]\n",
    "time1 = [1.0644979476928711,2.025758981704712,4.634213924407959,  1.636758089065552]\n",
    "\n",
    "rmseq = [4.9377327742385111,4.7375226333952503, 4.5513682313927744,4.4476116604281062]\n",
    "time2 = [0.211525917053222, 0.4993929862976074,  1.7231628894805908, 4.27150821685791]\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('RMSE')\n",
    "plt.plot(time1, rmse1, 'b.', label='LBFGS')\n",
    "plt.plot(time2,rmseq,'r.', label='QR')\n",
    "plt.legend()\n",
    "plt.savefig('rmse-time.pdf', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
